# CoRe: Benchmarking LLMsâ€™ Code Reasoning Capabilities through Static Analysis Tasks

This repository contains the source code, prompts, and annotation data for CoRe, a benchmark designed to evaluate LLMsâ€™ code reasoning capabilities through static analysis tasks.

### ðŸ”— Links
- Dataset: https://huggingface.co/datasets/danningx/CoRe
- Website: https://corebench.github.io

  
### Repository Structure
```
.
â”œâ”€â”€ raw_annotation/       # Human-verified annotations across C/C++, Java, Python
â”‚   â”œâ”€â”€ C/
â”‚   â”‚   â”œâ”€â”€ code/         # Program source files
â”‚   â”‚   â”œâ”€â”€ label/        # Ground truth labels
â”‚   â”‚   â””â”€â”€ meta_scan_result.json  # Static info scan results
â”‚   â”œâ”€â”€ Java/
â”‚   â””â”€â”€ Python/
â”‚
â”œâ”€â”€ prompts/              # All prompts used for evaluation
â”‚   â”œâ”€â”€ control_{Lang}_{source|trace}.jsonl
â”‚   â”œâ”€â”€ data_{Lang}_{source|trace}.jsonl
â”‚   â””â”€â”€ infoflowdep_{Lang}_{source|trace}.jsonl
â”‚   # Lang = C, Java, Python
â”‚   # *_source.jsonl â€” prompts for source enumeration
â”‚   # *_trace.jsonl â€” prompts for pairwise queries (classification + trace)
â”‚
â”œâ”€â”€ scripts/              # Scripts for running and evaluating LLMs
â”‚   â”œâ”€â”€ run.py            # Run LLM experiments
â”‚   â”œâ”€â”€ eval.py           # Evaluate model outputs
â”‚   â”œâ”€â”€ parse_label.py    # Parse and structure label files
â”‚   â”œâ”€â”€ parse_response.py # Parse model outputs
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ ScanResults.py    # Process scan results
â”‚   â”œâ”€â”€ run.sh            # Example run script
â”‚   â””â”€â”€ eval.sh           # Example eval script
â”‚
â”œâ”€â”€ lite.json             # Metadata for CoRe Lite subset

```


### Contents

- `raw_annotation/`: Contains annotated programs in three languages (C/C++, Java, Python). Each contains:
    - `code/`: Source files
    - `label/`: Ground truth annotations
    - `meta_scan_result.json`: Static analysis results with structural info for target sampling and filtering
- `prompts/`: Contains prompt templates for each task type and language.
    - Files are named as: `{task}_{language}_{mode}.jsonl`
        - `{task}`: data, control, or infoflowdep
        - `{language}`: C, Java, Python
        - `{mode}`: source for source enumeration, trace for pairwise classification and trace
- `scripts/`:
    - `run.py`: Main runner for LLM experiments
    - `eval.py`: Evaluation script
    - `parse_label.py`, parse_response.py: Utilities for parsing labels and LLM responses
    - `run.sh`, `eval.sh`: Example scripts demonstrating usage
- `lite.json`: Defines the list of task IDs included in CoRe Lite, a smaller representative subset of the full benchmark.


### Usage

To run or evaluate models, see the example shell scripts:


```
bash scripts/run.sh
bash scripts/eval.sh
```
Before running, please double check the scripts and update the paths and arguments.


Or use the CLI:

```
python scripts/run.py --help
python scripts/eval.py --help
```

To evaluate only on CoRe Lite, use the `--lite lite.json` argument when running `run.py`.

